2025-02-18 20:01:53.938393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1739908913.950074 1708809 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1739908913.953705 1708809 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-02-18 20:01:53.970105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
I0000 00:00:1739908917.838478 1708809 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20732 MB memory:  -> device: 0, name: NVIDIA A10, pci bus id: 0000:25:00.0, compute capability: 8.6
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1739908919.720307 1708853 service.cc:148] XLA service 0x7f4098001ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1739908919.720624 1708853 service.cc:156]   StreamExecutor device (0): NVIDIA A10, Compute Capability 8.6
2025-02-18 20:01:59.769554: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1739908919.921900 1708853 cuda_dnn.cc:529] Loaded cuDNN version 90501
2025-02-18 20:02:03.198881: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_1', 40 bytes spill stores, 40 bytes spill loads

I0000 00:00:1739908923.216611 1708853 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2025-02-18 20:02:25.956687: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: DATA_LOSS: truncated record at 11389505118' failed with Read less bytes than requested
	 [[{{node IteratorGetNext}}]]
2025-02-18 20:02:25.957191: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: DATA_LOSS: truncated record at 11389505118' failed with Read less bytes than requested
	 [[{{node IteratorGetNext}}]]
	 [[IteratorGetNext/_2]]
2025-02-18 20:02:25.957249: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 3754786005276782604
2025-02-18 20:02:25.957283: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 8237550439850426632
Traceback (most recent call last):
  File "/dcs/21/u2104990/Documents/324/runondcs/run.py", line 103, in <module>
    model.fit(
  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/tensorflow/python/eager/execute.py", line 53, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:

Detected at node IteratorGetNext defined at (most recent call last):
  File "/dcs/21/u2104990/Documents/324/runondcs/run.py", line 103, in <module>

  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py", line 371, in fit

  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py", line 219, in function

  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py", line 132, in multi_step_on_iterator

Detected at node IteratorGetNext defined at (most recent call last):
  File "/dcs/21/u2104990/Documents/324/runondcs/run.py", line 103, in <module>

  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py", line 117, in error_handler

  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py", line 371, in fit

  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py", line 219, in function

  File "/dcs/21/u2104990/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py", line 132, in multi_step_on_iterator

2 root error(s) found.
  (0) DATA_LOSS:  truncated record at 11389505118' failed with Read less bytes than requested
	 [[{{node IteratorGetNext}}]]
	 [[IteratorGetNext/_2]]
  (1) DATA_LOSS:  truncated record at 11389505118' failed with Read less bytes than requested
	 [[{{node IteratorGetNext}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_multi_step_on_iterator_2658]
